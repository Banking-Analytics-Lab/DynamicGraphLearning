{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d5b84db6-69fe-48d4-994d-ec68d5b2a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "510ebcc6-f1b8-4aae-b6f3-f84e80a28c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_list, i_list, ts_list, label_list = [], [], [], []\n",
    "feat_l = []\n",
    "idx_list = []\n",
    "data_name = './data/wikipedia.csv'\n",
    "def process(data_name): \n",
    "    with open(data_name) as f:\n",
    "        s = next(f)\n",
    "        for idx, line in enumerate(f):\n",
    "          e = line.strip().split(',')\n",
    "          #user id \n",
    "          u = int(e[0])\n",
    "          #item id \n",
    "          i = int(e[1])\n",
    "          #time stamp \n",
    "          ts = float(e[2])\n",
    "          #state label/ label of what it is \n",
    "          label = float(e[3])  # int(e[3])\n",
    "          #csv of features anything after index 4 \n",
    "          #note features must be numeric so we would 1 hot encode categorical variables\n",
    "          feat = np.array([float(x) for x in e[4:]])\n",
    "\n",
    "          u_list.append(u)\n",
    "          i_list.append(i)\n",
    "          ts_list.append(ts)\n",
    "          label_list.append(label)\n",
    "          idx_list.append(idx)\n",
    "\n",
    "          feat_l.append(feat)\n",
    "    return pd.DataFrame({'u': u_list,'i': i_list,'ts': ts_list,'label': label_list,'idx': idx_list}), np.array(feat_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "168a6b71-00ae-4154-b41b-702ece00de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex(df, bipartite=True):\n",
    "  new_df = df.copy()\n",
    "  if bipartite:\n",
    "    assert (df.u.max() - df.u.min() + 1 == len(df.u.unique()))\n",
    "    assert (df.i.max() - df.i.min() + 1 == len(df.i.unique()))\n",
    "\n",
    "    upper_u = df.u.max() + 1\n",
    "    new_i = df.i + upper_u\n",
    "\n",
    "    new_df.i = new_i\n",
    "    new_df.u += 1\n",
    "    new_df.i += 1\n",
    "    new_df.idx += 1\n",
    "  else:\n",
    "    new_df.u += 1\n",
    "    new_df.i += 1\n",
    "    new_df.idx += 1\n",
    "  return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b56a3c4e-ff72-4fca-81bb-804cc80e2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Data:\n",
    "  def __init__(self, sources, destinations, timestamps, edge_idxs, labels):\n",
    "    self.sources = sources\n",
    "    self.destinations = destinations\n",
    "    self.timestamps = timestamps\n",
    "    self.edge_idxs = edge_idxs\n",
    "    self.labels = labels\n",
    "    self.n_interactions = len(sources)\n",
    "    self.unique_nodes = set(sources) | set(destinations)\n",
    "    self.n_unique_nodes = len(self.unique_nodes)\n",
    "\n",
    "\n",
    "def get_data_node_classification(dataset_name, use_validation=False):\n",
    "  ### Load data and train val test split\n",
    "  graph_df = pd.read_csv('./data/ml_{}.csv'.format(dataset_name))\n",
    "  edge_features = np.load('./data/ml_{}.npy'.format(dataset_name))\n",
    "  node_features = np.load('./data/ml_{}_node.npy'.format(dataset_name))\n",
    "\n",
    "  val_time, test_time = list(np.quantile(graph_df.ts, [0.70, 0.85]))\n",
    "\n",
    "  sources = graph_df.u.values\n",
    "  destinations = graph_df.i.values\n",
    "  edge_idxs = graph_df.idx.values\n",
    "  labels = graph_df.label.values\n",
    "  timestamps = graph_df.ts.values\n",
    "\n",
    "  random.seed(2020)\n",
    "\n",
    "  train_mask = timestamps <= val_time if use_validation else timestamps <= test_time\n",
    "  test_mask = timestamps > test_time\n",
    "  val_mask = np.logical_and(timestamps <= test_time, timestamps > val_time) if use_validation else test_mask\n",
    "\n",
    "  full_data = Data(sources, destinations, timestamps, edge_idxs, labels)\n",
    "\n",
    "  train_data = Data(sources[train_mask], destinations[train_mask], timestamps[train_mask],\n",
    "                    edge_idxs[train_mask], labels[train_mask])\n",
    "\n",
    "  val_data = Data(sources[val_mask], destinations[val_mask], timestamps[val_mask],\n",
    "                  edge_idxs[val_mask], labels[val_mask])\n",
    "\n",
    "  test_data = Data(sources[test_mask], destinations[test_mask], timestamps[test_mask],\n",
    "                   edge_idxs[test_mask], labels[test_mask])\n",
    "\n",
    "  return full_data, node_features, edge_features, train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def get_data(dataset_name, different_new_nodes_between_val_and_test=False, randomize_features=False):\n",
    "  ### Load data and train val test split\n",
    "  graph_df = pd.read_csv('./data/ml_{}.csv'.format(dataset_name))\n",
    "  edge_features = np.load('./data/ml_{}.npy'.format(dataset_name))\n",
    "  node_features = np.load('./data/ml_{}_node.npy'.format(dataset_name)) \n",
    "    \n",
    "  if randomize_features:\n",
    "    node_features = np.random.rand(node_features.shape[0], node_features.shape[1])\n",
    "\n",
    "  val_time, test_time = list(np.quantile(graph_df.ts, [0.70, 0.85]))\n",
    "\n",
    "  sources = graph_df.u.values\n",
    "  destinations = graph_df.i.values\n",
    "  edge_idxs = graph_df.idx.values\n",
    "  labels = graph_df.label.values\n",
    "  timestamps = graph_df.ts.values\n",
    "\n",
    "  full_data = Data(sources, destinations, timestamps, edge_idxs, labels)\n",
    "\n",
    "  random.seed(2020)\n",
    "\n",
    "  node_set = set(sources) | set(destinations)\n",
    "\n",
    "  n_total_unique_nodes = len(node_set)\n",
    "\n",
    "  # Compute nodes which appear at test time\n",
    "  test_node_set = set(sources[timestamps > val_time]).union(\n",
    "    set(destinations[timestamps > val_time]))\n",
    "  # Sample nodes which we keep as new nodes (to test inductiveness), so than we have to remove all\n",
    "  # their edges from training\n",
    "  new_test_node_set = set(random.sample(test_node_set, int(0.1 * n_total_unique_nodes)))\n",
    "\n",
    "  # Mask saying for each source and destination whether they are new test nodes\n",
    "  new_test_source_mask = graph_df.u.map(lambda x: x in new_test_node_set).values\n",
    "  new_test_destination_mask = graph_df.i.map(lambda x: x in new_test_node_set).values\n",
    "\n",
    "  # Mask which is true for edges with both destination and source not being new test nodes (because\n",
    "  # we want to remove all edges involving any new test node)\n",
    "  observed_edges_mask = np.logical_and(~new_test_source_mask, ~new_test_destination_mask)\n",
    "\n",
    "  # For train we keep edges happening before the validation time which do not involve any new node\n",
    "  # used for inductiveness\n",
    "  train_mask = np.logical_and(timestamps <= val_time, observed_edges_mask)\n",
    "\n",
    "  train_data = Data(sources[train_mask], destinations[train_mask], timestamps[train_mask],\n",
    "                    edge_idxs[train_mask], labels[train_mask])\n",
    "\n",
    "  # define the new nodes sets for testing inductiveness of the model\n",
    "  train_node_set = set(train_data.sources).union(train_data.destinations)\n",
    "  m = 0\n",
    "  for i in train_node_set: \n",
    "        for j in new_test_node_set: \n",
    "            if i ==j:\n",
    "                print(j)\n",
    "            else: \n",
    "                m+= 1 \n",
    "  print(m)\n",
    "  # no cross over between sets \n",
    "  assert len(train_node_set & new_test_node_set) == 0\n",
    "  new_node_set = node_set - train_node_set\n",
    "\n",
    "  val_mask = np.logical_and(timestamps <= test_time, timestamps > val_time)\n",
    "  test_mask = timestamps > test_time\n",
    "\n",
    "  if different_new_nodes_between_val_and_test:\n",
    "    n_new_nodes = len(new_test_node_set) // 2\n",
    "    val_new_node_set = set(list(new_test_node_set)[:n_new_nodes])\n",
    "    test_new_node_set = set(list(new_test_node_set)[n_new_nodes:])\n",
    "\n",
    "    edge_contains_new_val_node_mask = np.array(\n",
    "      [(a in val_new_node_set or b in val_new_node_set) for a, b in zip(sources, destinations)])\n",
    "    edge_contains_new_test_node_mask = np.array(\n",
    "      [(a in test_new_node_set or b in test_new_node_set) for a, b in zip(sources, destinations)])\n",
    "    new_node_val_mask = np.logical_and(val_mask, edge_contains_new_val_node_mask)\n",
    "    new_node_test_mask = np.logical_and(test_mask, edge_contains_new_test_node_mask)\n",
    "\n",
    "\n",
    "  else:\n",
    "    edge_contains_new_node_mask = np.array(\n",
    "      [(a in new_node_set or b in new_node_set) for a, b in zip(sources, destinations)])\n",
    "    new_node_val_mask = np.logical_and(val_mask, edge_contains_new_node_mask)\n",
    "    new_node_test_mask = np.logical_and(test_mask, edge_contains_new_node_mask)\n",
    "\n",
    "  # validation and test with all edges\n",
    "  val_data = Data(sources[val_mask], destinations[val_mask], timestamps[val_mask],\n",
    "                  edge_idxs[val_mask], labels[val_mask])\n",
    "\n",
    "  test_data = Data(sources[test_mask], destinations[test_mask], timestamps[test_mask],\n",
    "                   edge_idxs[test_mask], labels[test_mask])\n",
    "\n",
    "  # validation and test with edges that at least has one new node (not in training set)\n",
    "  new_node_val_data = Data(sources[new_node_val_mask], destinations[new_node_val_mask],\n",
    "                           timestamps[new_node_val_mask],\n",
    "                           edge_idxs[new_node_val_mask], labels[new_node_val_mask])\n",
    "\n",
    "  new_node_test_data = Data(sources[new_node_test_mask], destinations[new_node_test_mask],\n",
    "                            timestamps[new_node_test_mask], edge_idxs[new_node_test_mask],\n",
    "                            labels[new_node_test_mask])\n",
    "\n",
    "  print(\"The dataset has {} interactions, involving {} different nodes\".format(full_data.n_interactions,\n",
    "                                                                      full_data.n_unique_nodes))\n",
    "  print(\"The training dataset has {} interactions, involving {} different nodes\".format(\n",
    "    train_data.n_interactions, train_data.n_unique_nodes))\n",
    "  print(\"The validation dataset has {} interactions, involving {} different nodes\".format(\n",
    "    val_data.n_interactions, val_data.n_unique_nodes))\n",
    "  print(\"The test dataset has {} interactions, involving {} different nodes\".format(\n",
    "    test_data.n_interactions, test_data.n_unique_nodes))\n",
    "  print(\"The new node validation dataset has {} interactions, involving {} different nodes\".format(\n",
    "    new_node_val_data.n_interactions, new_node_val_data.n_unique_nodes))\n",
    "  print(\"The new node test dataset has {} interactions, involving {} different nodes\".format(\n",
    "    new_node_test_data.n_interactions, new_node_test_data.n_unique_nodes))\n",
    "  print(\"{} nodes were used for the inductive testing, i.e. are never seen during training\".format(\n",
    "    len(new_test_node_set)))\n",
    "\n",
    "  return node_features, edge_features, full_data, train_data, val_data, test_data, \\\n",
    "         new_node_val_data, new_node_test_data\n",
    "\n",
    "\n",
    "def compute_time_statistics(sources, destinations, timestamps):\n",
    "  last_timestamp_sources = dict()\n",
    "  last_timestamp_dst = dict()\n",
    "  all_timediffs_src = []\n",
    "  all_timediffs_dst = []\n",
    "  for k in range(len(sources)):\n",
    "    source_id = sources[k]\n",
    "    dest_id = destinations[k]\n",
    "    c_timestamp = timestamps[k]\n",
    "    if source_id not in last_timestamp_sources.keys():\n",
    "      last_timestamp_sources[source_id] = 0\n",
    "    if dest_id not in last_timestamp_dst.keys():\n",
    "      last_timestamp_dst[dest_id] = 0\n",
    "    all_timediffs_src.append(c_timestamp - last_timestamp_sources[source_id])\n",
    "    all_timediffs_dst.append(c_timestamp - last_timestamp_dst[dest_id])\n",
    "    last_timestamp_sources[source_id] = c_timestamp\n",
    "    last_timestamp_dst[dest_id] = c_timestamp\n",
    "  assert len(all_timediffs_src) == len(sources)\n",
    "  assert len(all_timediffs_dst) == len(sources)\n",
    "  mean_time_shift_src = np.mean(all_timediffs_src)\n",
    "  std_time_shift_src = np.std(all_timediffs_src)\n",
    "  mean_time_shift_dst = np.mean(all_timediffs_dst)\n",
    "  std_time_shift_dst = np.std(all_timediffs_dst)\n",
    "\n",
    "  return mean_time_shift_src, std_time_shift_src, mean_time_shift_dst, std_time_shift_dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fc45e2e0-7263-4afd-8ab2-9b2442611599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_finder(data, uniform, max_node_idx=None):\n",
    "  max_node_idx = max(data.sources.max(), data.destinations.max()) if max_node_idx is None else max_node_idx\n",
    "  adj_list = [[] for _ in range(max_node_idx + 1)]\n",
    "  for source, destination, edge_idx, timestamp in zip(data.sources, data.destinations,\n",
    "                                                      data.edge_idxs,\n",
    "                                                      data.timestamps):\n",
    "    adj_list[source].append((destination, edge_idx, timestamp))\n",
    "    adj_list[destination].append((source, edge_idx, timestamp))\n",
    "  return NeighborFinder(adj_list, uniform=uniform)\n",
    "\n",
    "\n",
    "class NeighborFinder:\n",
    "  def __init__(self, adj_list, uniform=False, seed=None):\n",
    "    self.node_to_neighbors = []\n",
    "    self.node_to_edge_idxs = []\n",
    "    self.node_to_edge_timestamps = []\n",
    "\n",
    "    for neighbors in adj_list:\n",
    "      # Neighbors is a list of tuples (neighbor, edge_idx, timestamp)\n",
    "      # We sort the list based on timestamp\n",
    "      sorted_neighhbors = sorted(neighbors, key=lambda x: x[2])\n",
    "      self.node_to_neighbors.append(np.array([x[0] for x in sorted_neighhbors]))\n",
    "      self.node_to_edge_idxs.append(np.array([x[1] for x in sorted_neighhbors]))\n",
    "      self.node_to_edge_timestamps.append(np.array([x[2] for x in sorted_neighhbors]))\n",
    "\n",
    "    self.uniform = uniform\n",
    "\n",
    "    if seed is not None:\n",
    "      self.seed = seed\n",
    "      self.random_state = np.random.RandomState(self.seed)\n",
    "\n",
    "  def find_before(self, src_idx, cut_time):\n",
    "    \"\"\"\n",
    "    Extracts all the interactions happening before cut_time for user src_idx in the overall interaction graph. The returned interactions are sorted by time.\n",
    "\n",
    "    Returns 3 listxs: neighbors, edge_idxs, timestamps\n",
    "\n",
    "    \"\"\"\n",
    "    i = np.searchsorted(self.node_to_edge_timestamps[src_idx], cut_time)\n",
    "\n",
    "    return self.node_to_neighbors[src_idx][:i], self.node_to_edge_idxs[src_idx][:i], self.node_to_edge_timestamps[src_idx][:i]\n",
    "\n",
    "  def get_temporal_neighbor(self, source_nodes, timestamps, n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Given a list of users ids and relative cut times, extracts a sampled temporal neighborhood of each user in the list.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    src_idx_l: List[int]\n",
    "    cut_time_l: List[float],\n",
    "    num_neighbors: int\n",
    "    \"\"\"\n",
    "    assert (len(source_nodes) == len(timestamps))\n",
    "\n",
    "    tmp_n_neighbors = n_neighbors if n_neighbors > 0 else 1\n",
    "    # NB! All interactions described in these matrices are sorted in each row by time\n",
    "    neighbors = np.zeros((len(source_nodes), tmp_n_neighbors)).astype(\n",
    "      np.int32)  # each entry in position (i,j) represent the id of the item targeted by user src_idx_l[i] with an interaction happening before cut_time_l[i]\n",
    "    edge_times = np.zeros((len(source_nodes), tmp_n_neighbors)).astype(\n",
    "      np.float32)  # each entry in position (i,j) represent the timestamp of an interaction between user src_idx_l[i] and item neighbors[i,j] happening before cut_time_l[i]\n",
    "    edge_idxs = np.zeros((len(source_nodes), tmp_n_neighbors)).astype(\n",
    "      np.int32)  # each entry in position (i,j) represent the interaction index of an interaction between user src_idx_l[i] and item neighbors[i,j] happening before cut_time_l[i]\n",
    "\n",
    "    for i, (source_node, timestamp) in enumerate(zip(source_nodes, timestamps)):\n",
    "      source_neighbors, source_edge_idxs, source_edge_times = self.find_before(source_node,\n",
    "                                                   timestamp)  # extracts all neighbors, interactions indexes and timestamps of all interactions of user source_node happening before cut_time\n",
    "\n",
    "      if len(source_neighbors) > 0 and n_neighbors > 0:\n",
    "        if self.uniform:  # if we are applying uniform sampling, shuffles the data above before sampling\n",
    "          sampled_idx = np.random.randint(0, len(source_neighbors), n_neighbors)\n",
    "\n",
    "          neighbors[i, :] = source_neighbors[sampled_idx]\n",
    "          edge_times[i, :] = source_edge_times[sampled_idx]\n",
    "          edge_idxs[i, :] = source_edge_idxs[sampled_idx]\n",
    "\n",
    "          # re-sort based on time\n",
    "          pos = edge_times[i, :].argsort()\n",
    "          neighbors[i, :] = neighbors[i, :][pos]\n",
    "          edge_times[i, :] = edge_times[i, :][pos]\n",
    "          edge_idxs[i, :] = edge_idxs[i, :][pos]\n",
    "        else:\n",
    "          # Take most recent interactions\n",
    "          source_edge_times = source_edge_times[-n_neighbors:]\n",
    "          source_neighbors = source_neighbors[-n_neighbors:]\n",
    "          source_edge_idxs = source_edge_idxs[-n_neighbors:]\n",
    "\n",
    "          assert (len(source_neighbors) <= n_neighbors)\n",
    "          assert (len(source_edge_times) <= n_neighbors)\n",
    "          assert (len(source_edge_idxs) <= n_neighbors)\n",
    "\n",
    "          neighbors[i, n_neighbors - len(source_neighbors):] = source_neighbors\n",
    "          edge_times[i, n_neighbors - len(source_edge_times):] = source_edge_times\n",
    "          edge_idxs[i, n_neighbors - len(source_edge_idxs):] = source_edge_idxs\n",
    "\n",
    "    return neighbors, edge_idxs, edge_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4c76fe32-351b-4de6-82f1-a1602df89fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandEdgeSampler(object):\n",
    "  def __init__(self, src_list, dst_list, seed=None):\n",
    "    self.seed = None\n",
    "    self.src_list = np.unique(src_list)\n",
    "    self.dst_list = np.unique(dst_list)\n",
    "\n",
    "    if seed is not None:\n",
    "      self.seed = seed\n",
    "      self.random_state = np.random.RandomState(self.seed)\n",
    "\n",
    "  def sample(self, size):\n",
    "    if self.seed is None:\n",
    "      src_index = np.random.randint(0, len(self.src_list), size)\n",
    "      dst_index = np.random.randint(0, len(self.dst_list), size)\n",
    "    else:\n",
    "\n",
    "      src_index = self.random_state.randint(0, len(self.src_list), size)\n",
    "      dst_index = self.random_state.randint(0, len(self.dst_list), size)\n",
    "    return self.src_list[src_index], self.dst_list[dst_index]\n",
    "\n",
    "  def reset_random_state(self):\n",
    "    self.random_state = np.random.RandomState(self.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "99093882-4a15-4b15-8f63-213768d1b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_statistics(sources, destinations, timestamps):\n",
    "  last_timestamp_sources = dict()\n",
    "  last_timestamp_dst = dict()\n",
    "  all_timediffs_src = []\n",
    "  all_timediffs_dst = []\n",
    "  p = 0 \n",
    "  for k in range(len(sources)):\n",
    "    source_id = sources[k]\n",
    "    dest_id = destinations[k]\n",
    "    c_timestamp = timestamps[k]\n",
    "    if source_id not in last_timestamp_sources.keys():\n",
    "      last_timestamp_sources[source_id] = 0\n",
    "    if dest_id not in last_timestamp_dst.keys():\n",
    "      last_timestamp_dst[dest_id] = 0\n",
    "    all_timediffs_src.append(c_timestamp - last_timestamp_sources[source_id])\n",
    "    all_timediffs_dst.append(c_timestamp - last_timestamp_dst[dest_id])\n",
    "    last_timestamp_sources[source_id] = c_timestamp\n",
    "    last_timestamp_dst[dest_id] = c_timestamp\n",
    "  assert len(all_timediffs_src) == len(sources)\n",
    "  assert len(all_timediffs_dst) == len(sources)\n",
    "  mean_time_shift_src = np.mean(all_timediffs_src)\n",
    "  std_time_shift_src = np.std(all_timediffs_src)\n",
    "  mean_time_shift_dst = np.mean(all_timediffs_dst)\n",
    "  std_time_shift_dst = np.std(all_timediffs_dst)\n",
    "\n",
    "  return mean_time_shift_src, std_time_shift_src, mean_time_shift_dst, std_time_shift_dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "de6b4906-f5e4-430b-af62-896693a08679",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class TGN(torch.nn.Module):\n",
    "  def __init__(self, neighbor_finder, node_features, edge_features, device, n_layers=2,\n",
    "               n_heads=2, dropout=0.1, use_memory=False,\n",
    "               memory_update_at_start=True, message_dimension=100,\n",
    "               memory_dimension=500, embedding_module_type=\"graph_attention\",\n",
    "               message_function=\"mlp\",\n",
    "               mean_time_shift_src=0, std_time_shift_src=1, mean_time_shift_dst=0,\n",
    "               std_time_shift_dst=1, n_neighbors=None, aggregator_type=\"last\",\n",
    "               memory_updater_type=\"gru\",\n",
    "               use_destination_embedding_in_message=False,\n",
    "               use_source_embedding_in_message=False,\n",
    "               dyrep=False):\n",
    "    super(TGN, self).__init__()\n",
    "\n",
    "    self.n_layers = n_layers\n",
    "    self.neighbor_finder = neighbor_finder\n",
    "    self.device = device\n",
    "\n",
    "    self.node_raw_features = torch.from_numpy(node_features.astype(np.float32)).to(device)\n",
    "    self.edge_raw_features = torch.from_numpy(edge_features.astype(np.float32)).to(device)\n",
    "\n",
    "    self.n_node_features = self.node_raw_features.shape[1]\n",
    "    self.n_nodes = self.node_raw_features.shape[0]\n",
    "    self.n_edge_features = self.edge_raw_features.shape[1]\n",
    "    self.embedding_dimension = self.n_node_features\n",
    "    self.n_neighbors = n_neighbors\n",
    "    self.embedding_module_type = embedding_module_type\n",
    "    self.use_destination_embedding_in_message = use_destination_embedding_in_message\n",
    "    # self.use_source_embedding_in_message = use_source_embedding_in_message\n",
    "    # use false or won't use gru will use attention embeddings\n",
    "    self.use_source_embedding_in_message = False\n",
    "    self.dyrep = dyrep\n",
    "\n",
    "    self.use_memory = use_memory\n",
    "    self.time_encoder = TimeEncode(dimension=self.n_node_features)\n",
    "    self.memory = None\n",
    "\n",
    "    self.mean_time_shift_src = mean_time_shift_src\n",
    "    self.std_time_shift_src = std_time_shift_src\n",
    "    self.mean_time_shift_dst = mean_time_shift_dst\n",
    "    self.std_time_shift_dst = std_time_shift_dst\n",
    "\n",
    "    if self.use_memory:\n",
    "      self.memory_dimension = memory_dimension\n",
    "      #set default to true to match paper \n",
    "      self.memory_update_at_start = memory_update_at_start\n",
    "      #raw message dimension equals the same as concatication of \n",
    "      #memory state of node j, memory of node i, time embedding dimension, edge features\n",
    "      raw_message_dimension = 2 * self.memory_dimension + self.n_edge_features + \\\n",
    "                              self.time_encoder.dimension\n",
    "      message_dimension = message_dimension if message_function != \"identity\" else raw_message_dimension\n",
    "      self.memory = Memory(n_nodes=self.n_nodes,\n",
    "                           memory_dimension=self.memory_dimension,\n",
    "                           input_dimension=message_dimension,\n",
    "                           message_dimension=message_dimension,\n",
    "                           device=device)\n",
    "      self.message_aggregator = get_message_aggregator(aggregator_type=aggregator_type,\n",
    "                                                       device=device)\n",
    "      self.message_function = get_message_function(module_type=message_function,\n",
    "                                                   raw_message_dimension=raw_message_dimension,\n",
    "                                                   message_dimension=message_dimension)\n",
    "      self.memory_updater = get_memory_updater(module_type=memory_updater_type,\n",
    "                                               memory=self.memory,\n",
    "                                                 message_dimension=message_dimension,\n",
    "                                               memory_dimension=self.memory_dimension,\n",
    "                                               device=device)\n",
    "\n",
    "    self.embedding_module_type = embedding_module_type\n",
    "\n",
    "    self.embedding_module = get_embedding_module(module_type=embedding_module_type,\n",
    "                                                 node_features=self.node_raw_features,\n",
    "                                                 edge_features=self.edge_raw_features,\n",
    "                                                 memory=self.memory,\n",
    "                                                 neighbor_finder=self.neighbor_finder,\n",
    "                                                 time_encoder=self.time_encoder,\n",
    "                                                 n_layers=self.n_layers,\n",
    "                                                 n_node_features=self.n_node_features,\n",
    "                                                 n_edge_features=self.n_edge_features,\n",
    "                                                 n_time_features=self.n_node_features,\n",
    "                                                 embedding_dimension=self.embedding_dimension,\n",
    "                                                 device=self.device,\n",
    "                                                 n_heads=n_heads, dropout=dropout,\n",
    "                                                 use_memory=use_memory,\n",
    "                                                 n_neighbors=self.n_neighbors)\n",
    "\n",
    "    # MLP to compute probability on an edge given two node embeddings\n",
    "    self.affinity_score = MergeLayer(self.n_node_features, self.n_node_features,\n",
    "                                     self.n_node_features,\n",
    "                                     1)\n",
    "\n",
    "  def compute_temporal_embeddings(self, source_nodes, destination_nodes, negative_nodes, edge_times,\n",
    "                                  edge_idxs, n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Compute temporal embeddings for sources, destinations, and negatively sampled destinations.\n",
    "\n",
    "    source_nodes [batch_size]: source ids.\n",
    "    :param destination_nodes [batch_size]: destination ids\n",
    "    :param negative_nodes [batch_size]: ids of negative sampled destination\n",
    "    :param edge_times [batch_size]: timestamp of interaction\n",
    "    :param edge_idxs [batch_size]: index of interaction\n",
    "    :param n_neighbors [scalar]: number of temporal neighbor to consider in each convolutional\n",
    "    layer\n",
    "    :return: Temporal embeddings for sources, destinations and negatives\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = len(source_nodes)\n",
    "    nodes = np.concatenate([source_nodes, destination_nodes, negative_nodes])\n",
    "    positives = np.concatenate([source_nodes, destination_nodes])\n",
    "    timestamps = np.concatenate([edge_times, edge_times, edge_times])\n",
    "\n",
    "    memory = None\n",
    "    time_diffs = None\n",
    "    print(f'''{source_nodes.shape=}, {positives.shape=}, {negative_nodes.shape=},                                               ''')\n",
    "    #    if self.use_memory\n",
    "    if True:\n",
    "        # if self.memory_update_at_start:\n",
    "      if True:\n",
    "        # Update memory for all nodes with messages stored in previous batches\n",
    "        # findme1 \n",
    "        memory, last_update = self.get_updated_memory(list(range(self.n_nodes)),\n",
    "                                                      self.memory.messages)\n",
    "      else:\n",
    "        memory = self.memory.get_memory(list(range(self.n_nodes)))\n",
    "        last_update = self.memory.last_update\n",
    "\n",
    "      ### Compute differences between the time the memory of a node was last updated,\n",
    "      ### and the time for which we want to compute the embedding of a node\n",
    "      source_time_diffs = torch.LongTensor(edge_times).to(self.device) - last_update[\n",
    "        source_nodes].long()\n",
    "      source_time_diffs = (source_time_diffs - self.mean_time_shift_src) / self.std_time_shift_src\n",
    "      destination_time_diffs = torch.LongTensor(edge_times).to(self.device) - last_update[\n",
    "        destination_nodes].long()\n",
    "      destination_time_diffs = (destination_time_diffs - self.mean_time_shift_dst) / self.std_time_shift_dst\n",
    "      negative_time_diffs = torch.LongTensor(edge_times).to(self.device) - last_update[\n",
    "        negative_nodes].long()\n",
    "      negative_time_diffs = (negative_time_diffs - self.mean_time_shift_dst) / self.std_time_shift_dst\n",
    "\n",
    "      time_diffs = torch.cat([source_time_diffs, destination_time_diffs, negative_time_diffs],\n",
    "                             dim=0)\n",
    "\n",
    "    # Compute the embeddings using the embedding module\n",
    "    node_embedding = self.embedding_module.compute_embedding(memory=memory,\n",
    "                                                             source_nodes=nodes,\n",
    "                                                             timestamps=timestamps,\n",
    "                                                             n_layers=self.n_layers,\n",
    "                                                             n_neighbors=n_neighbors,\n",
    "                                                             time_diffs=time_diffs)\n",
    "\n",
    "    source_node_embedding = node_embedding[:n_samples]\n",
    "    destination_node_embedding = node_embedding[n_samples: 2 * n_samples]\n",
    "    negative_node_embedding = node_embedding[2 * n_samples:]\n",
    "\n",
    "    if self.use_memory:\n",
    "      if self.memory_update_at_start:\n",
    "        # Persist the updates to the memory only for sources and destinations (since now we have\n",
    "        # new messages for them)\n",
    "        # on the first pass it updates memory to zeros/does nothing since we have no message stores\n",
    "        self.update_memory(positives, self.memory.messages)\n",
    "\n",
    "        assert torch.allclose(memory[positives], self.memory.get_memory(positives), atol=1e-5), \\\n",
    "          \"Something wrong in how the memory was updated\"\n",
    "\n",
    "        # Remove messages for the positives since we have already updated the memory using them\n",
    "        self.memory.clear_messages(positives)\n",
    "\n",
    "      unique_sources, source_id_to_messages = self.get_raw_messages(source_nodes,\n",
    "                                                                    source_node_embedding,\n",
    "                                                                    destination_nodes,\n",
    "                                                                    destination_node_embedding,\n",
    "                                                                    edge_times, edge_idxs)\n",
    "      unique_destinations, destination_id_to_messages = self.get_raw_messages(destination_nodes,\n",
    "                                                                              destination_node_embedding,\n",
    "                                                                              source_nodes,\n",
    "                                                                              source_node_embedding,\n",
    "                                                                              edge_times, edge_idxs)\n",
    "      if self.memory_update_at_start:\n",
    "        self.memory.store_raw_messages(unique_sources, source_id_to_messages)\n",
    "        self.memory.store_raw_messages(unique_destinations, destination_id_to_messages)\n",
    "      else:\n",
    "        self.update_memory(unique_sources, source_id_to_messages)\n",
    "        self.update_memory(unique_destinations, destination_id_to_messages)\n",
    "\n",
    "      if self.dyrep:\n",
    "        source_node_embedding = memory[source_nodes]\n",
    "        destination_node_embedding = memory[destination_nodes]\n",
    "        negative_node_embedding = memory[negative_nodes]\n",
    "\n",
    "    return source_node_embedding, destination_node_embedding, negative_node_embedding\n",
    "\n",
    "  def compute_edge_probabilities(self, source_nodes, destination_nodes, negative_nodes, edge_times,\n",
    "                                 edge_idxs, n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Compute probabilities for edges between sources and destination and between sources and\n",
    "    negatives by first computing temporal embeddings using the TGN encoder and then feeding them\n",
    "    into the MLP decoder.\n",
    "    :param destination_nodes [batch_size]: destination ids\n",
    "    :param negative_nodes [batch_size]: ids of negative sampled destination\n",
    "    :param edge_times [batch_size]: timestamp of interaction\n",
    "    :param edge_idxs [batch_size]: index of interaction\n",
    "    :param n_neighbors [scalar]: number of temporal neighbor to consider in each convolutional\n",
    "    layer\n",
    "    :return: Probabilities for both the positive and negative edges\n",
    "    \"\"\"\n",
    "    n_samples = len(source_nodes)\n",
    "    \n",
    "    source_node_embedding, destination_node_embedding, negative_node_embedding = self.compute_temporal_embeddings(\n",
    "      source_nodes, destination_nodes, negative_nodes, edge_times, edge_idxs, n_neighbors)\n",
    "\n",
    "    score = self.affinity_score(torch.cat([source_node_embedding, source_node_embedding], dim=0),\n",
    "                                torch.cat([destination_node_embedding,\n",
    "                                           negative_node_embedding])).squeeze(dim=0)\n",
    "    pos_score = score[:n_samples]\n",
    "    neg_score = score[n_samples:]\n",
    "\n",
    "    return pos_score.sigmoid(), neg_score.sigmoid()\n",
    "\n",
    "  def update_memory(self, nodes, messages):\n",
    "    # Aggregate messages for the same nodes\n",
    "    unique_nodes, unique_messages, unique_timestamps = \\\n",
    "      self.message_aggregator.aggregate(\n",
    "        nodes,\n",
    "        messages)\n",
    "\n",
    "    if len(unique_nodes) > 0:\n",
    "      unique_messages = self.message_function.compute_message(unique_messages)\n",
    "\n",
    "    # Update the memory with the aggregated messages\n",
    "    self.memory_updater.update_memory(unique_nodes, unique_messages,\n",
    "                                      timestamps=unique_timestamps)\n",
    "\n",
    "  def get_updated_memory(self, nodes, messages):\n",
    "    # Aggregate messages for the same nodes\n",
    "    #findme2 \n",
    "    unique_nodes, unique_messages, unique_timestamps = \\\n",
    "      self.message_aggregator.aggregate(\n",
    "        nodes,\n",
    "        messages)\n",
    "    print(f'========GET UPDATED MEMORY=======\\n {len(unique_nodes)=}')\n",
    "    '''\n",
    "    ========================================FIND ME ============================================================\n",
    "    ========================================FIND ME ============================================================\n",
    "    ========================================FIND ME ============================================================\n",
    "    ========================================FIND ME ============================================================\n",
    "    ========================================FIND ME ============================================================\n",
    "    ========================================FIND ME ============================================================\n",
    "    ========================================FIND ME ============================================================\n",
    "    ========================================FIND ME ============================================================\n",
    "    ========================================FIND ME ============================================================\n",
    "    ========================================FIND ME ============================================================\n",
    "    ========================================FIND ME ==========================================================\n",
    "\n",
    "'''\n",
    "    if len(unique_nodes) > 0:\n",
    "      #we are using identity function so so just returns raw messages\n",
    "      unique_messages = self.message_function.compute_message(unique_messages)\n",
    "    \n",
    "    # MEMORY UPDATOR METHOD TO GET UPDATED MEMORY NOT TGA METHOD \n",
    "    updated_memory, updated_last_update = self.memory_updater.get_updated_memory(unique_nodes,\n",
    "                                                                                 unique_messages,\n",
    "                                                                                 timestamps=unique_timestamps)\n",
    "\n",
    "    return updated_memory, updated_last_update\n",
    "\n",
    "  def get_raw_messages(self, source_nodes, source_node_embedding, destination_nodes,\n",
    "                       destination_node_embedding, edge_times, edge_idxs):\n",
    "\n",
    "    #  MAKE SURE THIS IS FALSE IN FUNCTION CALL \n",
    "    # self.use_source_embedding_in_message                   \n",
    "    edge_times = torch.from_numpy(edge_times).float().to(self.device)\n",
    "    edge_features = self.edge_raw_features[edge_idxs]\n",
    "\n",
    "    source_memory = self.memory.get_memory(source_nodes) if not \\\n",
    "      self.use_source_embedding_in_message else source_node_embedding\n",
    "    destination_memory = self.memory.get_memory(destination_nodes) if \\\n",
    "      not self.use_destination_embedding_in_message else destination_node_embedding\n",
    "\n",
    "    source_time_delta = edge_times - self.memory.last_update[source_nodes]\n",
    "    source_time_delta_encoding = self.time_encoder(source_time_delta.unsqueeze(dim=1)).view(len(\n",
    "      source_nodes), -1)\n",
    "\n",
    "    source_message = torch.cat([source_memory, destination_memory, edge_features,\n",
    "                                source_time_delta_encoding],\n",
    "                               dim=1)\n",
    "    messages = defaultdict(list)\n",
    "    unique_sources = np.unique(source_nodes)\n",
    "\n",
    "    for i in range(len(source_nodes)):\n",
    "      messages[source_nodes[i]].append((source_message[i], edge_times[i]))\n",
    "\n",
    "    return unique_sources, messages\n",
    "\n",
    "  def set_neighbor_finder(self, neighbor_finder):\n",
    "    self.neighbor_finder = neighbor_finder\n",
    "    self.embedding_module.neighbor_finder = neighbor_finder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5d336c0c-dcc2-4cf1-a064-5aef1eb22813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class Memory(nn.Module):\n",
    "\n",
    "  def __init__(self, n_nodes, memory_dimension, input_dimension, message_dimension=None,\n",
    "               device=\"cpu\", combination_method='sum'):\n",
    "    super(Memory, self).__init__()\n",
    "    self.n_nodes = n_nodes\n",
    "    self.memory_dimension = memory_dimension\n",
    "    self.input_dimension = input_dimension\n",
    "    self.message_dimension = message_dimension\n",
    "    self.device = device\n",
    "\n",
    "    self.combination_method = combination_method\n",
    "\n",
    "    self.__init_memory__()\n",
    "\n",
    "  def __init_memory__(self):\n",
    "    \"\"\"\n",
    "    Initializes the memory to all zeros. It should be called at the start of each epoch.\n",
    "    \"\"\"\n",
    "    # Treat memory as parameter so that it is saved and loaded together with the model\n",
    "    self.memory = nn.Parameter(torch.zeros((self.n_nodes, self.memory_dimension)).to(self.device),\n",
    "                               requires_grad=False)\n",
    "    print(f'{self.n_nodes=}, {self.memory_dimension=}')\n",
    "    self.last_update = nn.Parameter(torch.zeros(self.n_nodes).to(self.device),\n",
    "                                    requires_grad=False)\n",
    "\n",
    "    self.messages = defaultdict(list)\n",
    "\n",
    "  def store_raw_messages(self, nodes, node_id_to_messages):\n",
    "    for node in nodes:\n",
    "      self.messages[node].extend(node_id_to_messages[node])\n",
    "\n",
    "  def get_memory(self, node_idxs):\n",
    "    return self.memory[node_idxs, :]\n",
    "\n",
    "  def set_memory(self, node_idxs, values):\n",
    "    self.memory[node_idxs, :] = values\n",
    "\n",
    "  def get_last_update(self, node_idxs):\n",
    "    return self.last_update[node_idxs]\n",
    "\n",
    "  def backup_memory(self):\n",
    "    messages_clone = {}\n",
    "    for k, v in self.messages.items():\n",
    "      messages_clone[k] = [(x[0].clone(), x[1].clone()) for x in v]\n",
    "\n",
    "    return self.memory.data.clone(), self.last_update.data.clone(), messages_clone\n",
    "\n",
    "  def restore_memory(self, memory_backup):\n",
    "    self.memory.data, self.last_update.data = memory_backup[0].clone(), memory_backup[1].clone()\n",
    "\n",
    "    self.messages = defaultdict(list)\n",
    "    for k, v in memory_backup[2].items():\n",
    "      self.messages[k] = [(x[0].clone(), x[1].clone()) for x in v]\n",
    "\n",
    "  def detach_memory(self):\n",
    "    self.memory.detach_()\n",
    "\n",
    "    # Detach all stored messages\n",
    "    for k, v in self.messages.items():\n",
    "      new_node_messages = []\n",
    "      for message in v:\n",
    "        new_node_messages.append((message[0].detach(), message[1]))\n",
    "\n",
    "      self.messages[k] = new_node_messages\n",
    "\n",
    "  def clear_messages(self, nodes):\n",
    "    for node in nodes:\n",
    "      self.messages[node] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ff415201-1c6c-4ff0-8595-e4154c5f2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TimeEncode(torch.nn.Module):\n",
    "  # Time Encoding proposed by TGAT\n",
    "  def __init__(self, dimension):\n",
    "    super(TimeEncode, self).__init__()\n",
    "\n",
    "    self.dimension = dimension\n",
    "    self.w = torch.nn.Linear(1, dimension)\n",
    "\n",
    "    self.w.weight = torch.nn.Parameter((torch.from_numpy(1 / 10 ** np.linspace(0, 9, dimension)))\n",
    "                                       .float().reshape(dimension, -1))\n",
    "    self.w.bias = torch.nn.Parameter(torch.zeros(dimension).float())\n",
    "\n",
    "  def forward(self, t):\n",
    "    # t has shape [batch_size, seq_len]\n",
    "    # Add dimension at the end to apply linear layer --> [batch_size, seq_len, 1]\n",
    "    t = t.unsqueeze(dim=2)\n",
    "\n",
    "    # output has shape [batch_size, seq_len, dimension]\n",
    "    output = torch.cos(self.w(t))\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8680804f-5bbc-4a37-8bef-14c59a147e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MessageAggregator(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  Abstract class for the message aggregator module, which given a batch of node ids and\n",
    "  corresponding messages, aggregates messages with the same node id.\n",
    "  \"\"\"\n",
    "  def __init__(self, device):\n",
    "    super(MessageAggregator, self).__init__()\n",
    "    self.device = device\n",
    "\n",
    "  def aggregate(self, node_ids, messages):\n",
    "    \"\"\"\n",
    "    Given a list of node ids, and a list of messages of the same length, aggregate different\n",
    "    messages for the same id using one of the possible strategies.\n",
    "    :param node_ids: A list of node ids of length batch_size\n",
    "    :param messages: A tensor of shape [batch_size, message_length]\n",
    "    :param timestamps A tensor of shape [batch_size]\n",
    "    :return: A tensor of shape [n_unique_node_ids, message_length] with the aggregated messages\n",
    "    \"\"\"\n",
    "\n",
    "  def group_by_id(self, node_ids, messages, timestamps):\n",
    "    node_id_to_messages = defaultdict(list)\n",
    "\n",
    "    for i, node_id in enumerate(node_ids):\n",
    "      node_id_to_messages[node_id].append((messages[i], timestamps[i]))\n",
    "\n",
    "    return node_id_to_messages\n",
    "\n",
    "\n",
    "class LastMessageAggregator(MessageAggregator):\n",
    "  def __init__(self, device):\n",
    "    super(LastMessageAggregator, self).__init__(device)\n",
    "\n",
    "  def aggregate(self, node_ids, messages):\n",
    "    \"\"\"Only keep the last message for each node\"\"\"    \n",
    "    unique_node_ids = np.unique(node_ids)\n",
    "    unique_messages = []\n",
    "    unique_timestamps = []\n",
    "    \n",
    "    to_update_node_ids = []\n",
    "    \n",
    "    for node_id in unique_node_ids:\n",
    "        if len(messages[node_id]) > 0:\n",
    "            to_update_node_ids.append(node_id)\n",
    "            unique_messages.append(messages[node_id][-1][0])\n",
    "            unique_timestamps.append(messages[node_id][-1][1])\n",
    "    \n",
    "    unique_messages = torch.stack(unique_messages) if len(to_update_node_ids) > 0 else []\n",
    "    unique_timestamps = torch.stack(unique_timestamps) if len(to_update_node_ids) > 0 else []\n",
    "\n",
    "    return to_update_node_ids, unique_messages, unique_timestamps\n",
    "\n",
    "'''\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ==========================================================\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class MeanMessageAggregator(MessageAggregator):\n",
    "  def __init__(self, device):\n",
    "    super(MeanMessageAggregator, self).__init__(device)\n",
    "\n",
    "  def aggregate(self, node_ids, messages):\n",
    "    \"\"\"Only keep the last message for each node\"\"\"\n",
    "    \n",
    "    unique_node_ids = np.unique(node_ids)\n",
    "    unique_messages = []\n",
    "    unique_timestamps = []\n",
    "\n",
    "    to_update_node_ids = []\n",
    "    n_messages = 0\n",
    "    #findme3\n",
    "    print(f'========= MEAN MESSAGE AGG =======\\n  ')\n",
    "    for node_id in unique_node_ids:\n",
    "\n",
    "      if len(messages[node_id]) > 0:\n",
    "        n_messages += len(messages[node_id])\n",
    "        to_update_node_ids.append(node_id)\n",
    "        unique_messages.append(torch.mean(torch.stack([m[0] for m in messages[node_id]]), dim=0))\n",
    "        unique_timestamps.append(messages[node_id][-1][1])\n",
    "        # print(f'{n_messages=} {len(to_update_node_ids)=}  {len(unique_messages)=} ')\n",
    "\n",
    "    unique_messages = torch.stack(unique_messages) if len(to_update_node_ids) > 0 else []\n",
    "    unique_timestamps = torch.stack(unique_timestamps) if len(to_update_node_ids) > 0 else []\n",
    "    # toprint1 = unique_messages.shape if unique_messages else [] \n",
    "    # toprint2 =unique_timestamps.shape if unique_timestamps else []\n",
    "    # print(f'{toprint1=} { toprint2=}')\n",
    "    return to_update_node_ids, unique_messages, unique_timestamps\n",
    "'''\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ============================================================\n",
    "========================================FIND ME ==========================================================\n",
    "\n",
    "'''\n",
    "\n",
    "def get_message_aggregator(aggregator_type, device):\n",
    "  if aggregator_type == \"last\":\n",
    "    return LastMessageAggregator(device=device)\n",
    "  elif aggregator_type == \"mean\":\n",
    "    return MeanMessageAggregator(device=device)\n",
    "  else:\n",
    "    raise ValueError(\"Message aggregator {} not implemented\".format(aggregator_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7fd2f4c4-ff55-412d-a5c2-2dcd285bea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MessageFunction(nn.Module):\n",
    "  \"\"\"\n",
    "  Module which computes the message for a given interaction.\n",
    "  \"\"\"\n",
    "\n",
    "  def compute_message(self, raw_messages):\n",
    "    return None\n",
    "\n",
    "\n",
    "class MLPMessageFunction(MessageFunction):\n",
    "  def __init__(self, raw_message_dimension, message_dimension):\n",
    "    super(MLPMessageFunction, self).__init__()\n",
    "\n",
    "    self.mlp = self.layers = nn.Sequential(\n",
    "      nn.Linear(raw_message_dimension, raw_message_dimension // 2),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(raw_message_dimension // 2, message_dimension),\n",
    "    )\n",
    "\n",
    "  def compute_message(self, raw_messages):\n",
    "    messages = self.mlp(raw_messages)\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "class IdentityMessageFunction(MessageFunction):\n",
    "\n",
    "  def compute_message(self, raw_messages):\n",
    "\n",
    "    return raw_messages\n",
    "\n",
    "\n",
    "def get_message_function(module_type, raw_message_dimension, message_dimension):\n",
    "  if module_type == \"mlp\":\n",
    "    return MLPMessageFunction(raw_message_dimension, message_dimension)\n",
    "  elif module_type == \"identity\":\n",
    "    return IdentityMessageFunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "07ef99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TimeEncode(torch.nn.Module):\n",
    "  # Time Encoding proposed by TGAT\n",
    "  def __init__(self, dimension):\n",
    "    super(TimeEncode, self).__init__()\n",
    "\n",
    "    self.dimension = dimension\n",
    "    self.w = torch.nn.Linear(1, dimension)\n",
    "\n",
    "    self.w.weight = torch.nn.Parameter((torch.from_numpy(1 / 10 ** np.linspace(0, 9, dimension)))\n",
    "                                       .float().reshape(dimension, -1))\n",
    "    self.w.bias = torch.nn.Parameter(torch.zeros(dimension).float())\n",
    "\n",
    "  def forward(self, t):\n",
    "    # t has shape [batch_size, seq_len]\n",
    "    # Add dimension at the end to apply linear layer --> [batch_size, seq_len, 1]\n",
    "    t = t.unsqueeze(dim=2)\n",
    "\n",
    "    # output has shape [batch_size, seq_len, dimension]\n",
    "    output = torch.cos(self.w(t))\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6842d970-74d5-40dc-9958-2e0088506eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class MemoryUpdater(nn.Module):\n",
    "  def update_memory(self, unique_node_ids, unique_messages, timestamps):\n",
    "    pass\n",
    "\n",
    "\n",
    "class SequenceMemoryUpdater(MemoryUpdater):\n",
    "  def __init__(self, memory, message_dimension, memory_dimension, device):\n",
    "    super(SequenceMemoryUpdater, self).__init__()\n",
    "    self.memory = memory\n",
    "    self.layer_norm = torch.nn.LayerNorm(memory_dimension)\n",
    "    self.message_dimension = message_dimension\n",
    "    self.device = device\n",
    "\n",
    "  def update_memory(self, unique_node_ids, unique_messages, timestamps):\n",
    "    # unique messages are raw messages, ie simple concatanation of old source memory, old destination memory \n",
    "    # time between update time and last edge appereance \n",
    "    # and edge features \n",
    "    if len(unique_node_ids) <= 0:\n",
    "      return\n",
    "\n",
    "    assert (self.memory.get_last_update(unique_node_ids) <= timestamps).all().item(), \"Trying to \" \\\n",
    "                                                                                     \"update memory to time in the past\"\n",
    "\n",
    "    memory = self.memory.get_memory(unique_node_ids)\n",
    "    self.memory.last_update[unique_node_ids] = timestamps\n",
    "\n",
    "    updated_memory = self.memory_updater(unique_messages, memory)\n",
    "\n",
    "    self.memory.set_memory(unique_node_ids, updated_memory)\n",
    "\n",
    "  def get_updated_memory(self, unique_node_ids, unique_messages, timestamps):\n",
    "    if len(unique_node_ids) <= 0:\n",
    "      return self.memory.memory.data.clone(), self.memory.last_update.data.clone()\n",
    "\n",
    "    assert (self.memory.get_last_update(unique_node_ids) <= timestamps).all().item(), \"Trying to \" \\\n",
    "                                                                                     \"update memory to time in the past\"\n",
    "\n",
    "    updated_memory = self.memory.memory.data.clone()\n",
    "    print(f'{updated_memory=}')\n",
    "    print('EXIT ==================================================')\n",
    "    exit(0)\n",
    "    '''\n",
    "    FIND ME =========================\n",
    "    FIND ME =========================\n",
    "    FIND ME =========================\n",
    "    FIND ME =========================\n",
    "    FIND ME =========================\n",
    "    FIND ME =========================\n",
    "    '''\n",
    "    updated_memory[unique_node_ids] = self.memory_updater(unique_messages, updated_memory[unique_node_ids])\n",
    "\n",
    "    updated_last_update = self.memory.last_update.data.clone()\n",
    "    updated_last_update[unique_node_ids] = timestamps\n",
    "\n",
    "    return updated_memory, updated_last_update\n",
    "\n",
    "\n",
    "class GRUMemoryUpdater(SequenceMemoryUpdater):\n",
    "  def __init__(self, memory, message_dimension, memory_dimension, device):\n",
    "    super(GRUMemoryUpdater, self).__init__(memory, message_dimension, memory_dimension, device)\n",
    "\n",
    "    self.memory_updater = nn.GRUCell(input_size=message_dimension,\n",
    "                                     hidden_size=memory_dimension)\n",
    "\n",
    "\n",
    "class RNNMemoryUpdater(SequenceMemoryUpdater):\n",
    "  def __init__(self, memory, message_dimension, memory_dimension, device):\n",
    "    super(RNNMemoryUpdater, self).__init__(memory, message_dimension, memory_dimension, device)\n",
    "\n",
    "    self.memory_updater = nn.RNNCell(input_size=message_dimension,\n",
    "                                     hidden_size=memory_dimension)\n",
    "\n",
    "\n",
    "def get_memory_updater(module_type, memory, message_dimension, memory_dimension, device):\n",
    "  if module_type == \"gru\":\n",
    "    return GRUMemoryUpdater(memory, message_dimension, memory_dimension, device)\n",
    "  elif module_type == \"rnn\":\n",
    "    return RNNMemoryUpdater(memory, message_dimension, memory_dimension, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "79968bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class TemporalAttentionLayer(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  Temporal attention layer. Return the temporal embedding of a node given the node itself,\n",
    "   its neighbors and the edge timestamps.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, n_node_features, n_neighbors_features, n_edge_features, time_dim,\n",
    "               output_dimension, n_head=2,\n",
    "               dropout=0.1):\n",
    "    super(TemporalAttentionLayer, self).__init__()\n",
    "\n",
    "    self.n_head = n_head\n",
    "\n",
    "    self.feat_dim = n_node_features\n",
    "    self.time_dim = time_dim\n",
    "\n",
    "    self.query_dim = n_node_features + time_dim\n",
    "    self.key_dim = n_neighbors_features + time_dim + n_edge_features\n",
    "\n",
    "    self.merger = MergeLayer(self.query_dim, n_node_features, n_node_features, output_dimension)\n",
    "\n",
    "    self.multi_head_target = nn.MultiheadAttention(embed_dim=self.query_dim,\n",
    "                                                   kdim=self.key_dim,\n",
    "                                                   vdim=self.key_dim,\n",
    "                                                   num_heads=n_head,\n",
    "                                                   dropout=dropout)\n",
    "\n",
    "  def forward(self, src_node_features, src_time_features, neighbors_features,\n",
    "              neighbors_time_features, edge_features, neighbors_padding_mask):\n",
    "    \"\"\"\n",
    "    \"Temporal attention model\n",
    "    :param src_node_features: float Tensor of shape [batch_size, n_node_features]\n",
    "    :param src_time_features: float Tensor of shape [batch_size, 1, time_dim]\n",
    "    :param neighbors_features: float Tensor of shape [batch_size, n_neighbors, n_node_features]\n",
    "    :param neighbors_time_features: float Tensor of shape [batch_size, n_neighbors,\n",
    "    time_dim]\n",
    "    :param edge_features: float Tensor of shape [batch_size, n_neighbors, n_edge_features]\n",
    "    :param neighbors_padding_mask: float Tensor of shape [batch_size, n_neighbors]\n",
    "    :return:\n",
    "    attn_output: float Tensor of shape [1, batch_size, n_node_features]\n",
    "    attn_output_weights: [batch_size, 1, n_neighbors]\n",
    "    \"\"\"\n",
    "\n",
    "    src_node_features_unrolled = torch.unsqueeze(src_node_features, dim=1)\n",
    "\n",
    "    query = torch.cat([src_node_features_unrolled, src_time_features], dim=2)\n",
    "    key = torch.cat([neighbors_features, edge_features, neighbors_time_features], dim=2)\n",
    "\n",
    "    # print(neighbors_features.shape, edge_features.shape, neighbors_time_features.shape)\n",
    "    # Reshape tensors so to expected shape by multi head attention\n",
    "    query = query.permute([1, 0, 2])  # [1, batch_size, num_of_features]\n",
    "    key = key.permute([1, 0, 2])  # [n_neighbors, batch_size, num_of_features]\n",
    "\n",
    "    # Compute mask of which source nodes have no valid neighbors\n",
    "    invalid_neighborhood_mask = neighbors_padding_mask.all(dim=1, keepdim=True)\n",
    "    # If a source node has no valid neighbor, set it's first neighbor to be valid. This will\n",
    "    # force the attention to just 'attend' on this neighbor (which has the same features as all\n",
    "    # the others since they are fake neighbors) and will produce an equivalent result to the\n",
    "    # original tgat paper which was forcing fake neighbors to all have same attention of 1e-10\n",
    "    neighbors_padding_mask[invalid_neighborhood_mask.squeeze(), 0] = False\n",
    "\n",
    "    # print(query.shape, key.shape)\n",
    "\n",
    "    attn_output, attn_output_weights = self.multi_head_target(query=query, key=key, value=key,\n",
    "                                                              key_padding_mask=neighbors_padding_mask)\n",
    "\n",
    "    # mask = torch.unsqueeze(neighbors_padding_mask, dim=2)  # mask [B, N, 1]\n",
    "    # mask = mask.permute([0, 2, 1])\n",
    "    # attn_output, attn_output_weights = self.multi_head_target(q=query, k=key, v=key,\n",
    "    #                                                           mask=mask)\n",
    "\n",
    "    attn_output = attn_output.squeeze()\n",
    "    attn_output_weights = attn_output_weights.squeeze()\n",
    "\n",
    "    # Source nodes with no neighbors have an all zero attention output. The attention output is\n",
    "    # then added or concatenated to the original source node features and then fed into an MLP.\n",
    "    # This means that an all zero vector is not used.\n",
    "    attn_output = attn_output.masked_fill(invalid_neighborhood_mask, 0)\n",
    "    attn_output_weights = attn_output_weights.masked_fill(invalid_neighborhood_mask, 0)\n",
    "\n",
    "    # Skip connection with temporal attention over neighborhood and the features of the node itself\n",
    "    attn_output = self.merger(attn_output, src_node_features)\n",
    "\n",
    "    return attn_output, attn_output_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3fec7884-d72f-4145-8cae-8f15d794d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from model.temporal_attention import TemporalAttentionLayer\n",
    "\n",
    "\n",
    "class EmbeddingModule(nn.Module):\n",
    "  def __init__(self, node_features, edge_features, memory, neighbor_finder, time_encoder, n_layers,\n",
    "               n_node_features, n_edge_features, n_time_features, embedding_dimension, device,\n",
    "               dropout):\n",
    "    super(EmbeddingModule, self).__init__()\n",
    "    self.node_features = node_features\n",
    "    self.edge_features = edge_features\n",
    "    # self.memory = memory\n",
    "    self.neighbor_finder = neighbor_finder\n",
    "    self.time_encoder = time_encoder\n",
    "    self.n_layers = n_layers\n",
    "    self.n_node_features = n_node_features\n",
    "    self.n_edge_features = n_edge_features\n",
    "    self.n_time_features = n_time_features\n",
    "    self.dropout = dropout\n",
    "    self.embedding_dimension = embedding_dimension\n",
    "    self.device = device\n",
    "\n",
    "  def compute_embedding(self, memory, source_nodes, timestamps, n_layers, n_neighbors=20, time_diffs=None,\n",
    "                        use_time_proj=True):\n",
    "    pass\n",
    "\n",
    "\n",
    "class IdentityEmbedding(EmbeddingModule):\n",
    "  def compute_embedding(self, memory, source_nodes, timestamps, n_layers, n_neighbors=20, time_diffs=None,\n",
    "                        use_time_proj=True):\n",
    "    return memory[source_nodes, :]\n",
    "\n",
    "\n",
    "class TimeEmbedding(EmbeddingModule):\n",
    "  def __init__(self, node_features, edge_features, memory, neighbor_finder, time_encoder, n_layers,\n",
    "               n_node_features, n_edge_features, n_time_features, embedding_dimension, device,\n",
    "               n_heads=2, dropout=0.1, use_memory=True, n_neighbors=1):\n",
    "    super(TimeEmbedding, self).__init__(node_features, edge_features, memory,\n",
    "                                        neighbor_finder, time_encoder, n_layers,\n",
    "                                        n_node_features, n_edge_features, n_time_features,\n",
    "                                        embedding_dimension, device, dropout)\n",
    "\n",
    "    class NormalLinear(nn.Linear):\n",
    "      # From Jodie code\n",
    "      def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.normal_(0, stdv)\n",
    "        if self.bias is not None:\n",
    "          self.bias.data.normal_(0, stdv)\n",
    "\n",
    "    self.embedding_layer = NormalLinear(1, self.n_node_features)\n",
    "\n",
    "  def compute_embedding(self, memory, source_nodes, timestamps, n_layers, n_neighbors=20, time_diffs=None,\n",
    "                        use_time_proj=True):\n",
    "    source_embeddings = memory[source_nodes, :] * (1 + self.embedding_layer(time_diffs.unsqueeze(1)))\n",
    "\n",
    "    return source_embeddings\n",
    "\n",
    "\n",
    "class GraphEmbedding(EmbeddingModule):\n",
    "  def __init__(self, node_features, edge_features, memory, neighbor_finder, time_encoder, n_layers,\n",
    "               n_node_features, n_edge_features, n_time_features, embedding_dimension, device,\n",
    "               n_heads=2, dropout=0.1, use_memory=True):\n",
    "    super(GraphEmbedding, self).__init__(node_features, edge_features, memory,\n",
    "                                         neighbor_finder, time_encoder, n_layers,\n",
    "                                         n_node_features, n_edge_features, n_time_features,\n",
    "                                         embedding_dimension, device, dropout)\n",
    "\n",
    "    self.use_memory = use_memory\n",
    "    self.device = device\n",
    "\n",
    "  def compute_embedding(self, memory, source_nodes, timestamps, n_layers, n_neighbors=20, time_diffs=None,\n",
    "                        use_time_proj=True):\n",
    "    \"\"\"Recursive implementation of curr_layers temporal graph attention layers.\n",
    "\n",
    "    src_idx_l [batch_size]: users / items input ids.\n",
    "    cut_time_l [batch_size]: scalar representing the instant of the time where we want to extract the user / item representation.\n",
    "    curr_layers [scalar]: number of temporal convolutional layers to stack.\n",
    "    num_neighbors [scalar]: number of temporal neighbor to consider in each convolutional layer.\n",
    "    \"\"\"\n",
    "\n",
    "    assert (n_layers >= 0)\n",
    "\n",
    "    source_nodes_torch = torch.from_numpy(source_nodes).long().to(self.device)\n",
    "    timestamps_torch = torch.unsqueeze(torch.from_numpy(timestamps).float().to(self.device), dim=1)\n",
    "\n",
    "    # query node always has the start time -> time span == 0\n",
    "    source_nodes_time_embedding = self.time_encoder(torch.zeros_like(\n",
    "      timestamps_torch))\n",
    "\n",
    "    source_node_features = self.node_features[source_nodes_torch, :]\n",
    "\n",
    "    if self.use_memory:\n",
    "      source_node_features = memory[source_nodes, :] + source_node_features\n",
    "\n",
    "    if n_layers == 0:\n",
    "      return source_node_features\n",
    "    else:\n",
    "\n",
    "      neighbors, edge_idxs, edge_times = self.neighbor_finder.get_temporal_neighbor(\n",
    "        source_nodes,\n",
    "        timestamps,\n",
    "        n_neighbors=n_neighbors)\n",
    "\n",
    "      neighbors_torch = torch.from_numpy(neighbors).long().to(self.device)\n",
    "\n",
    "      edge_idxs = torch.from_numpy(edge_idxs).long().to(self.device)\n",
    "\n",
    "      edge_deltas = timestamps[:, np.newaxis] - edge_times\n",
    "\n",
    "      edge_deltas_torch = torch.from_numpy(edge_deltas).float().to(self.device)\n",
    "\n",
    "      neighbors = neighbors.flatten()\n",
    "      neighbor_embeddings = self.compute_embedding(memory,\n",
    "                                                   neighbors,\n",
    "                                                   np.repeat(timestamps, n_neighbors),\n",
    "                                                   n_layers=n_layers - 1,\n",
    "                                                   n_neighbors=n_neighbors)\n",
    "\n",
    "      effective_n_neighbors = n_neighbors if n_neighbors > 0 else 1\n",
    "      neighbor_embeddings = neighbor_embeddings.view(len(source_nodes), effective_n_neighbors, -1)\n",
    "      edge_time_embeddings = self.time_encoder(edge_deltas_torch)\n",
    "\n",
    "      edge_features = self.edge_features[edge_idxs, :]\n",
    "\n",
    "      mask = neighbors_torch == 0\n",
    "\n",
    "      source_embedding = self.aggregate(n_layers, source_node_features,\n",
    "                                        source_nodes_time_embedding,\n",
    "                                        neighbor_embeddings,\n",
    "                                        edge_time_embeddings,\n",
    "                                        edge_features,\n",
    "                                        mask)\n",
    "\n",
    "      return source_embedding\n",
    "\n",
    "  def aggregate(self, n_layers, source_node_features, source_nodes_time_embedding,\n",
    "                neighbor_embeddings,\n",
    "                edge_time_embeddings, edge_features, mask):\n",
    "    return None\n",
    "\n",
    "\n",
    "class GraphSumEmbedding(GraphEmbedding):\n",
    "  def __init__(self, node_features, edge_features, memory, neighbor_finder, time_encoder, n_layers,\n",
    "               n_node_features, n_edge_features, n_time_features, embedding_dimension, device,\n",
    "               n_heads=2, dropout=0.1, use_memory=True):\n",
    "    super(GraphSumEmbedding, self).__init__(node_features=node_features,\n",
    "                                            edge_features=edge_features,\n",
    "                                            memory=memory,\n",
    "                                            neighbor_finder=neighbor_finder,\n",
    "                                            time_encoder=time_encoder, n_layers=n_layers,\n",
    "                                            n_node_features=n_node_features,\n",
    "                                            n_edge_features=n_edge_features,\n",
    "                                            n_time_features=n_time_features,\n",
    "                                            embedding_dimension=embedding_dimension,\n",
    "                                            device=device,\n",
    "                                            n_heads=n_heads, dropout=dropout,\n",
    "                                            use_memory=use_memory)\n",
    "    self.linear_1 = torch.nn.ModuleList([torch.nn.Linear(embedding_dimension + n_time_features +\n",
    "                                                         n_edge_features, embedding_dimension)\n",
    "                                         for _ in range(n_layers)])\n",
    "    self.linear_2 = torch.nn.ModuleList(\n",
    "      [torch.nn.Linear(embedding_dimension + n_node_features + n_time_features,\n",
    "                       embedding_dimension) for _ in range(n_layers)])\n",
    "\n",
    "  def aggregate(self, n_layer, source_node_features, source_nodes_time_embedding,\n",
    "                neighbor_embeddings,\n",
    "                edge_time_embeddings, edge_features, mask):\n",
    "    neighbors_features = torch.cat([neighbor_embeddings, edge_time_embeddings, edge_features],\n",
    "                                   dim=2)\n",
    "    neighbor_embeddings = self.linear_1[n_layer - 1](neighbors_features)\n",
    "    neighbors_sum = torch.nn.functional.relu(torch.sum(neighbor_embeddings, dim=1))\n",
    "\n",
    "    source_features = torch.cat([source_node_features,\n",
    "                                 source_nodes_time_embedding.squeeze()], dim=1)\n",
    "    source_embedding = torch.cat([neighbors_sum, source_features], dim=1)\n",
    "    source_embedding = self.linear_2[n_layer - 1](source_embedding)\n",
    "\n",
    "    return source_embedding\n",
    "\n",
    "\n",
    "class GraphAttentionEmbedding(GraphEmbedding):\n",
    "  def __init__(self, node_features, edge_features, memory, neighbor_finder, time_encoder, n_layers,\n",
    "               n_node_features, n_edge_features, n_time_features, embedding_dimension, device,\n",
    "               n_heads=2, dropout=0.1, use_memory=True):\n",
    "    super(GraphAttentionEmbedding, self).__init__(node_features, edge_features, memory,\n",
    "                                                  neighbor_finder, time_encoder, n_layers,\n",
    "                                                  n_node_features, n_edge_features,\n",
    "                                                  n_time_features,\n",
    "                                                  embedding_dimension, device,\n",
    "                                                  n_heads, dropout,\n",
    "                                                  use_memory)\n",
    "\n",
    "    self.attention_models = torch.nn.ModuleList([TemporalAttentionLayer(\n",
    "      n_node_features=n_node_features,\n",
    "      n_neighbors_features=n_node_features,\n",
    "      n_edge_features=n_edge_features,\n",
    "      time_dim=n_time_features,\n",
    "      n_head=n_heads,\n",
    "      dropout=dropout,\n",
    "      output_dimension=n_node_features)\n",
    "      for _ in range(n_layers)])\n",
    "\n",
    "  def aggregate(self, n_layer, source_node_features, source_nodes_time_embedding,\n",
    "                neighbor_embeddings,\n",
    "                edge_time_embeddings, edge_features, mask):\n",
    "    attention_model = self.attention_models[n_layer - 1]\n",
    "\n",
    "    source_embedding, _ = attention_model(source_node_features,\n",
    "                                          source_nodes_time_embedding,\n",
    "                                          neighbor_embeddings,\n",
    "                                          edge_time_embeddings,\n",
    "                                          edge_features,\n",
    "                                          mask)\n",
    "\n",
    "    return source_embedding\n",
    "\n",
    "\n",
    "def get_embedding_module(module_type, node_features, edge_features, memory, neighbor_finder,\n",
    "                         time_encoder, n_layers, n_node_features, n_edge_features, n_time_features,\n",
    "                         embedding_dimension, device,\n",
    "                         n_heads=2, dropout=0.1, n_neighbors=None,\n",
    "                         use_memory=True):\n",
    "  if module_type == \"graph_attention\":\n",
    "    return GraphAttentionEmbedding(node_features=node_features,\n",
    "                                    edge_features=edge_features,\n",
    "                                    memory=memory,\n",
    "                                    neighbor_finder=neighbor_finder,\n",
    "                                    time_encoder=time_encoder,\n",
    "                                    n_layers=n_layers,\n",
    "                                    n_node_features=n_node_features,\n",
    "                                    n_edge_features=n_edge_features,\n",
    "                                    n_time_features=n_time_features,\n",
    "                                    embedding_dimension=embedding_dimension,\n",
    "                                    device=device,\n",
    "                                    n_heads=n_heads, dropout=dropout, use_memory=use_memory)\n",
    "  elif module_type == \"graph_sum\":\n",
    "    return GraphSumEmbedding(node_features=node_features,\n",
    "                              edge_features=edge_features,\n",
    "                              memory=memory,\n",
    "                              neighbor_finder=neighbor_finder,\n",
    "                              time_encoder=time_encoder,\n",
    "                              n_layers=n_layers,\n",
    "                              n_node_features=n_node_features,\n",
    "                              n_edge_features=n_edge_features,\n",
    "                              n_time_features=n_time_features,\n",
    "                              embedding_dimension=embedding_dimension,\n",
    "                              device=device,\n",
    "                              n_heads=n_heads, dropout=dropout, use_memory=use_memory)\n",
    "\n",
    "  elif module_type == \"identity\":\n",
    "    return IdentityEmbedding(node_features=node_features,\n",
    "                             edge_features=edge_features,\n",
    "                             memory=memory,\n",
    "                             neighbor_finder=neighbor_finder,\n",
    "                             time_encoder=time_encoder,\n",
    "                             n_layers=n_layers,\n",
    "                             n_node_features=n_node_features,\n",
    "                             n_edge_features=n_edge_features,\n",
    "                             n_time_features=n_time_features,\n",
    "                             embedding_dimension=embedding_dimension,\n",
    "                             device=device,\n",
    "                             dropout=dropout)\n",
    "  elif module_type == \"time\":\n",
    "    return TimeEmbedding(node_features=node_features,\n",
    "                         edge_features=edge_features,\n",
    "                         memory=memory,\n",
    "                         neighbor_finder=neighbor_finder,\n",
    "                         time_encoder=time_encoder,\n",
    "                         n_layers=n_layers,\n",
    "                         n_node_features=n_node_features,\n",
    "                         n_edge_features=n_edge_features,\n",
    "                         n_time_features=n_time_features,\n",
    "                         embedding_dimension=embedding_dimension,\n",
    "                         device=device,\n",
    "                         dropout=dropout,\n",
    "                         n_neighbors=n_neighbors)\n",
    "  else:\n",
    "    raise ValueError(\"Embedding Module {} not supported\".format(module_type))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c1a3d780-c99b-4528-bb04-06a9eef15241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class MergeLayer(torch.nn.Module):\n",
    "  def __init__(self, dim1, dim2, dim3, dim4):\n",
    "    super().__init__()\n",
    "    self.fc1 = torch.nn.Linear(dim1 + dim2, dim3)\n",
    "    self.fc2 = torch.nn.Linear(dim3, dim4)\n",
    "    self.act = torch.nn.ReLU()\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "\n",
    "  def forward(self, x1, x2):\n",
    "    x = torch.cat([x1, x2], dim=1)\n",
    "    h = self.act(self.fc1(x))\n",
    "    return self.fc2(h)\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "  def __init__(self, dim, drop=0.3):\n",
    "    super().__init__()\n",
    "    self.fc_1 = torch.nn.Linear(dim, 80)\n",
    "    self.fc_2 = torch.nn.Linear(80, 10)\n",
    "    self.fc_3 = torch.nn.Linear(10, 1)\n",
    "    self.act = torch.nn.ReLU()\n",
    "    self.dropout = torch.nn.Dropout(p=drop, inplace=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.act(self.fc_1(x))\n",
    "    x = self.dropout(x)\n",
    "    x = self.act(self.fc_2(x))\n",
    "    x = self.dropout(x)\n",
    "    return self.fc_3(x).squeeze(dim=1)\n",
    "\n",
    "\n",
    "class EarlyStopMonitor(object):\n",
    "  def __init__(self, max_round=3, higher_better=True, tolerance=1e-10):\n",
    "    self.max_round = max_round\n",
    "    self.num_round = 0\n",
    "\n",
    "    self.epoch_count = 0\n",
    "    self.best_epoch = 0\n",
    "\n",
    "    self.last_best = None\n",
    "    self.higher_better = higher_better\n",
    "    self.tolerance = tolerance\n",
    "\n",
    "  def early_stop_check(self, curr_val):\n",
    "    if not self.higher_better:\n",
    "      curr_val *= -1\n",
    "    if self.last_best is None:\n",
    "      self.last_best = curr_val\n",
    "    elif (curr_val - self.last_best) / np.abs(self.last_best) > self.tolerance:\n",
    "      self.last_best = curr_val\n",
    "      self.num_round = 0\n",
    "      self.best_epoch = self.epoch_count\n",
    "    else:\n",
    "      self.num_round += 1\n",
    "\n",
    "    self.epoch_count += 1\n",
    "\n",
    "    return self.num_round >= self.max_round\n",
    "\n",
    "\n",
    "class RandEdgeSampler(object):\n",
    "  def __init__(self, src_list, dst_list, seed=None):\n",
    "    self.seed = None\n",
    "    self.src_list = np.unique(src_list)\n",
    "    self.dst_list = np.unique(dst_list)\n",
    "\n",
    "    if seed is not None:\n",
    "      self.seed = seed\n",
    "      self.random_state = np.random.RandomState(self.seed)\n",
    "\n",
    "  def sample(self, size):\n",
    "    if self.seed is None:\n",
    "      src_index = np.random.randint(0, len(self.src_list), size)\n",
    "      dst_index = np.random.randint(0, len(self.dst_list), size)\n",
    "    else:\n",
    "\n",
    "      src_index = self.random_state.randint(0, len(self.src_list), size)\n",
    "      dst_index = self.random_state.randint(0, len(self.dst_list), size)\n",
    "    return self.src_list[src_index], self.dst_list[dst_index]\n",
    "\n",
    "  def reset_random_state(self):\n",
    "    self.random_state = np.random.RandomState(self.seed)\n",
    "\n",
    "\n",
    "def get_neighbor_finder(data, uniform, max_node_idx=None):\n",
    "  max_node_idx = max(data.sources.max(), data.destinations.max()) if max_node_idx is None else max_node_idx\n",
    "  adj_list = [[] for _ in range(max_node_idx + 1)]\n",
    "  for source, destination, edge_idx, timestamp in zip(data.sources, data.destinations,\n",
    "                                                      data.edge_idxs,\n",
    "                                                      data.timestamps):\n",
    "    adj_list[source].append((destination, edge_idx, timestamp))\n",
    "    adj_list[destination].append((source, edge_idx, timestamp))\n",
    "\n",
    "  return NeighborFinder(adj_list, uniform=uniform)\n",
    "\n",
    "\n",
    "class NeighborFinder:\n",
    "  def __init__(self, adj_list, uniform=False, seed=None):\n",
    "    self.node_to_neighbors = []\n",
    "    self.node_to_edge_idxs = []\n",
    "    self.node_to_edge_timestamps = []\n",
    "\n",
    "    for neighbors in adj_list:\n",
    "      # Neighbors is a list of tuples (neighbor, edge_idx, timestamp)\n",
    "      # We sort the list based on timestamp\n",
    "      sorted_neighhbors = sorted(neighbors, key=lambda x: x[2])\n",
    "      self.node_to_neighbors.append(np.array([x[0] for x in sorted_neighhbors]))\n",
    "      self.node_to_edge_idxs.append(np.array([x[1] for x in sorted_neighhbors]))\n",
    "      self.node_to_edge_timestamps.append(np.array([x[2] for x in sorted_neighhbors]))\n",
    "\n",
    "    self.uniform = uniform\n",
    "\n",
    "    if seed is not None:\n",
    "      self.seed = seed\n",
    "      self.random_state = np.random.RandomState(self.seed)\n",
    "\n",
    "  def find_before(self, src_idx, cut_time):\n",
    "    \"\"\"\n",
    "    Extracts all the interactions happening before cut_time for user src_idx in the overall interaction graph. The returned interactions are sorted by time.\n",
    "\n",
    "    Returns 3 lists: neighbors, edge_idxs, timestamps\n",
    "\n",
    "    \"\"\"\n",
    "    i = np.searchsorted(self.node_to_edge_timestamps[src_idx], cut_time)\n",
    "\n",
    "    return self.node_to_neighbors[src_idx][:i], self.node_to_edge_idxs[src_idx][:i], self.node_to_edge_timestamps[src_idx][:i]\n",
    "\n",
    "  def get_temporal_neighbor(self, source_nodes, timestamps, n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Given a list of users ids and relative cut times, extracts a sampled temporal neighborhood of each user in the list.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    src_idx_l: List[int]\n",
    "    cut_time_l: List[float],\n",
    "    num_neighbors: int\n",
    "    \"\"\"\n",
    "    assert (len(source_nodes) == len(timestamps))\n",
    "\n",
    "    tmp_n_neighbors = n_neighbors if n_neighbors > 0 else 1\n",
    "    # NB! All interactions described in these matrices are sorted in each row by time\n",
    "    neighbors = np.zeros((len(source_nodes), tmp_n_neighbors)).astype(\n",
    "      np.int32)  # each entry in position (i,j) represent the id of the item targeted by user src_idx_l[i] with an interaction happening before cut_time_l[i]\n",
    "    edge_times = np.zeros((len(source_nodes), tmp_n_neighbors)).astype(\n",
    "      np.float32)  # each entry in position (i,j) represent the timestamp of an interaction between user src_idx_l[i] and item neighbors[i,j] happening before cut_time_l[i]\n",
    "    edge_idxs = np.zeros((len(source_nodes), tmp_n_neighbors)).astype(\n",
    "      np.int32)  # each entry in position (i,j) represent the interaction index of an interaction between user src_idx_l[i] and item neighbors[i,j] happening before cut_time_l[i]\n",
    "\n",
    "    for i, (source_node, timestamp) in enumerate(zip(source_nodes, timestamps)):\n",
    "      source_neighbors, source_edge_idxs, source_edge_times = self.find_before(source_node,\n",
    "                                                   timestamp)  # extracts all neighbors, interactions indexes and timestamps of all interactions of user source_node happening before cut_time\n",
    "\n",
    "      if len(source_neighbors) > 0 and n_neighbors > 0:\n",
    "        if self.uniform:  # if we are applying uniform sampling, shuffles the data above before sampling\n",
    "          sampled_idx = np.random.randint(0, len(source_neighbors), n_neighbors)\n",
    "\n",
    "          neighbors[i, :] = source_neighbors[sampled_idx]\n",
    "          edge_times[i, :] = source_edge_times[sampled_idx]\n",
    "          edge_idxs[i, :] = source_edge_idxs[sampled_idx]\n",
    "\n",
    "          # re-sort based on time\n",
    "          pos = edge_times[i, :].argsort()\n",
    "          neighbors[i, :] = neighbors[i, :][pos]\n",
    "          edge_times[i, :] = edge_times[i, :][pos]\n",
    "          edge_idxs[i, :] = edge_idxs[i, :][pos]\n",
    "        else:\n",
    "          # Take most recent interactions\n",
    "          source_edge_times = source_edge_times[-n_neighbors:]\n",
    "          source_neighbors = source_neighbors[-n_neighbors:]\n",
    "          source_edge_idxs = source_edge_idxs[-n_neighbors:]\n",
    "\n",
    "          assert (len(source_neighbors) <= n_neighbors)\n",
    "          assert (len(source_edge_times) <= n_neighbors)\n",
    "          assert (len(source_edge_idxs) <= n_neighbors)\n",
    "\n",
    "          neighbors[i, n_neighbors - len(source_neighbors):] = source_neighbors\n",
    "          edge_times[i, n_neighbors - len(source_edge_times):] = source_edge_times\n",
    "          edge_idxs[i, n_neighbors - len(source_edge_idxs):] = source_edge_idxs\n",
    "\n",
    "    return neighbors, edge_idxs, edge_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d508d5f-4a86-4e8b-a097-6e51a80aeca2",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "315dc2cd-c0e4-4612-8759-45008795da6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5662002\n",
      "The dataset has 157474 interactions, involving 9227 different nodes\n",
      "The training dataset has 81029 interactions, involving 6141 different nodes\n",
      "The validation dataset has 23621 interactions, involving 3256 different nodes\n",
      "The test dataset has 23621 interactions, involving 3564 different nodes\n",
      "The new node validation dataset has 12016 interactions, involving 2120 different nodes\n",
      "The new node test dataset has 11715 interactions, involving 2437 different nodes\n",
      "922 nodes were used for the inductive testing, i.e. are never seen during training\n",
      "self.n_nodes=9228, self.memory_dimension=172\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "node_features, edge_features, full_data, train_data, val_data, test_data, new_node_val_data, new_node_test_data\\\n",
    "            =get_data(\"wikipedia\")\n",
    "nbf = get_neighbor_finder(full_data, False)\n",
    "mean_time_shift_src, std_time_shift_src, mean_time_shift_dst, std_time_shift_dst = \\\n",
    "    compute_time_statistics(full_data.sources, full_data.destinations, full_data.timestamps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize training neighbor finder to retrieve temporal graph\n",
    "train_ngh_finder = get_neighbor_finder(train_data, True)\n",
    "\n",
    "# Initialize validation and test neighbor finder to retrieve temporal graph\n",
    "full_ngh_finder = get_neighbor_finder(full_data, True)\n",
    "\n",
    "# Initialize negative samplers. Set seeds for validation and testing so negatives are the same\n",
    "# across different runs\n",
    "# NB: in the inductive setting, negatives are sampled only amongst other new nodes\n",
    "train_rand_sampler = RandEdgeSampler(train_data.sources, train_data.destinations)\n",
    "val_rand_sampler = RandEdgeSampler(full_data.sources, full_data.destinations, seed=0)\n",
    "nn_val_rand_sampler = RandEdgeSampler(new_node_val_data.sources, new_node_val_data.destinations,\n",
    "                                      seed=1)\n",
    "test_rand_sampler = RandEdgeSampler(full_data.sources, full_data.destinations, seed=2)\n",
    "nn_test_rand_sampler = RandEdgeSampler(new_node_test_data.sources,\n",
    "                                       new_node_test_data.destinations,\n",
    "                                       seed=3)\n",
    "tgn = TGN(neighbor_finder=train_ngh_finder, node_features=node_features,\n",
    "            edge_features=edge_features, device='cpu',\n",
    "            n_layers=2,\n",
    "            n_heads=2, dropout=.2, use_memory=True,\n",
    "            message_dimension=100, memory_dimension=172,\n",
    "            memory_update_at_start= False,\n",
    "            embedding_module_type='graph_attention',\n",
    "            message_function='identity',\n",
    "            aggregator_type='mean',\n",
    "            memory_updater_type='gru',\n",
    "            n_neighbors=10,\n",
    "            mean_time_shift_src=mean_time_shift_src, std_time_shift_src=std_time_shift_src,\n",
    "            mean_time_shift_dst=mean_time_shift_dst, std_time_shift_dst=std_time_shift_dst,\n",
    "            use_destination_embedding_in_message=True,\n",
    "            use_source_embedding_in_message=True, \n",
    "            dyrep=True)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(tgn.parameters(), lr=1)\n",
    "\n",
    "\n",
    "num_instance = len(train_data.sources)\n",
    "num_batch = math.ceil(num_instance / 100)\n",
    "\n",
    "idx_list = np.arange(num_instance)\n",
    "\n",
    "new_nodes_val_aps = []\n",
    "val_aps = []\n",
    "epoch_times = []\n",
    "total_epoch_times = []\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9503bf48-a52c-4424-8f9f-3af816095bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.n_nodes=9228, self.memory_dimension=172\n",
      "start_idx=0 \n",
      " end_idx=100 \n",
      " timestamps_batch.shape=(100,) \n",
      " pos_label.shape=torch.Size([100]) \n",
      " neg_label.shape=torch.Size([100])\n",
      "        sources_batch.shape=(100,), destinations_batch.shape=(100,), negatives_batch.shape=(100,),\n",
      "                                                            timestamps_batch.shape=(100,), edge_idxs_batch.shape=(100,)\n",
      "        \n",
      "source_nodes.shape=(100,), positives.shape=(200,), negative_nodes.shape=(100,),                                               \n",
      "========= MEAN MESSAGE AGG =======\n",
      "  \n",
      "========GET UPDATED MEMORY=======\n",
      " len(unique_nodes)=0\n",
      "========= MEAN MESSAGE AGG =======\n",
      "  \n",
      "========= MEAN MESSAGE AGG =======\n",
      "  \n",
      "start_idx=100 \n",
      " end_idx=200 \n",
      " timestamps_batch.shape=(100,) \n",
      " pos_label.shape=torch.Size([100]) \n",
      " neg_label.shape=torch.Size([100])\n",
      "        sources_batch.shape=(100,), destinations_batch.shape=(100,), negatives_batch.shape=(100,),\n",
      "                                                            timestamps_batch.shape=(100,), edge_idxs_batch.shape=(100,)\n",
      "        \n",
      "source_nodes.shape=(100,), positives.shape=(200,), negative_nodes.shape=(100,),                                               \n",
      "========= MEAN MESSAGE AGG =======\n",
      "  \n",
      "========GET UPDATED MEMORY=======\n",
      " len(unique_nodes)=0\n",
      "========= MEAN MESSAGE AGG =======\n",
      "  \n",
      "========= MEAN MESSAGE AGG =======\n",
      "  \n",
      "self.n_nodes=9228, self.memory_dimension=172\n",
      "start_idx=0 \n",
      " end_idx=100 \n",
      " timestamps_batch.shape=(100,) \n",
      " pos_label.shape=torch.Size([100]) \n",
      " neg_label.shape=torch.Size([100])\n",
      "        sources_batch.shape=(100,), destinations_batch.shape=(100,), negatives_batch.shape=(100,),\n",
      "                                                            timestamps_batch.shape=(100,), edge_idxs_batch.shape=(100,)\n",
      "        \n",
      "source_nodes.shape=(100,), positives.shape=(200,), negative_nodes.shape=(100,),                                               \n",
      "========= MEAN MESSAGE AGG =======\n",
      "  \n",
      "========GET UPDATED MEMORY=======\n",
      " len(unique_nodes)=0\n",
      "========= MEAN MESSAGE AGG =======\n",
      "  \n",
      "========= MEAN MESSAGE AGG =======\n",
      "  \n",
      "start_idx=100 \n",
      " end_idx=200 \n",
      " timestamps_batch.shape=(100,) \n",
      " pos_label.shape=torch.Size([100]) \n",
      " neg_label.shape=torch.Size([100])\n",
      "        sources_batch.shape=(100,), destinations_batch.shape=(100,), negatives_batch.shape=(100,),\n",
      "                                                            timestamps_batch.shape=(100,), edge_idxs_batch.shape=(100,)\n",
      "        \n",
      "source_nodes.shape=(100,), positives.shape=(200,), negative_nodes.shape=(100,),                                               \n",
      "========= MEAN MESSAGE AGG =======\n",
      "  \n",
      "========GET UPDATED MEMORY=======\n",
      " len(unique_nodes)=0\n",
      "========= MEAN MESSAGE AGG =======\n",
      "  \n",
      "========= MEAN MESSAGE AGG =======\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "USE_MEMORY = True\n",
    "NUM_EPOCH = 2\n",
    "BATCH_SIZE=100\n",
    "for epoch in range(NUM_EPOCH):\n",
    "### Training\n",
    "\n",
    "# Reinitialize memory of the model at the start of each epoch\n",
    "    if USE_MEMORY:\n",
    "\n",
    "      tgn.memory.__init_memory__()\n",
    "\n",
    "    # Train using only training graph\n",
    "    tgn.set_neighbor_finder(train_ngh_finder)\n",
    "    m_loss = []\n",
    "#    for k in range(0, num_batch, args.backprop_every):\n",
    "    for k in range(0, 2,1 ):\n",
    "      loss = 0\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Custom loop to allow to perform backpropagation only every a certain number of batches\n",
    "      for j in range(1):\n",
    "        batch_idx = k + j\n",
    "\n",
    "        if batch_idx >= num_batch:\n",
    "          continue\n",
    "\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = min(num_instance, start_idx + BATCH_SIZE)\n",
    "        \n",
    "        sources_batch, destinations_batch = train_data.sources[start_idx:end_idx], \\\n",
    "                                            train_data.destinations[start_idx:end_idx]\n",
    "        edge_idxs_batch = train_data.edge_idxs[start_idx: end_idx]\n",
    "        timestamps_batch = train_data.timestamps[start_idx:end_idx]\n",
    "\n",
    "        size = len(sources_batch)\n",
    "        _, negatives_batch = train_rand_sampler.sample(size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "          pos_label = torch.ones(size, dtype=torch.float, device='cpu')\n",
    "          neg_label = torch.zeros(size, dtype=torch.float, device='cpu')\n",
    "        print(f'''{start_idx=} \\n {end_idx=} \\n {timestamps_batch.shape=} \\n {pos_label.shape=} \\n {neg_label.shape=}\n",
    "        {sources_batch.shape=}, {destinations_batch.shape=}, {negatives_batch.shape=},\n",
    "                                                            {timestamps_batch.shape=}, {edge_idxs_batch.shape=}\n",
    "        ''')\n",
    "        tgn = tgn.train()\n",
    "        pos_prob, neg_prob = tgn.compute_edge_probabilities(sources_batch, destinations_batch, negatives_batch,timestamps_batch, edge_idxs_batch, 10)\n",
    "\n",
    "        loss += criterion(pos_prob.squeeze(), pos_label) + criterion(neg_prob.squeeze(), neg_label)\n",
    "\n",
    "      loss /= 1\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      m_loss.append(loss.item())\n",
    "\n",
    "      # Detach memory after 'args.backprop_every' number of batches so we don't backpropagate to\n",
    "      # the start of time\n",
    "      if USE_MEMORY:\n",
    "        tgn.memory.detach_memory()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
